{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JustinGaj/verizon-1a-project-falcon/blob/main/cell_tower_mini_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mini model for getting rid of images without cell towers"
      ],
      "metadata": {
        "id": "rZpIted2L5iE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRGNaakpbkA1",
        "outputId": "83f5d4d9-dbb4-4105-d527-2293039e7763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.11.12)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.3)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.61.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.4)\n",
            "Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Bird-nest-dataset-1 to yolov8:: 100%|██████████| 156526/156526 [00:09<00:00, 15972.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Bird-nest-dataset-1 in yolov8:: 100%|██████████| 1256/1256 [00:00<00:00, 2449.72it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"ta7eF8BgO0KGqSxF1rpZ\")\n",
        "project = rf.workspace(\"kalynb700\").project(\"bird-nest-dataset-kpvdq-gc2uo-upzvj\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(num_classes=2)\n",
        "model.load_state_dict(torch.load('best_cell_tower_model.pth'))\n",
        "model.eval()  # Set to evaluation mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JNSLX3Rv_zO",
        "outputId": "d093a48d-c8ae-4f69-b99c-21bb017674c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=2048, out_features=256, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.3, inplace=False)\n",
              "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CellTowerDataset(Dataset):\n",
        "    \"\"\"Dataset for cell tower classification\"\"\"\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def parse_yolo_labels(label_path, cell_class_id, non_cell_class_ids):\n",
        "    \"\"\"Parse YOLO format labels to check if cell tower exists\"\"\"\n",
        "    if not os.path.exists(label_path):\n",
        "        return None\n",
        "\n",
        "    with open(label_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    if not lines:\n",
        "        return None\n",
        "\n",
        "    # Check ALL annotations in the file\n",
        "    has_cell_tower = False\n",
        "    has_non_cell = False\n",
        "    all_class_ids = []\n",
        "\n",
        "    for line in lines:\n",
        "        parts = line.strip().split()\n",
        "        if parts:\n",
        "            class_id = int(parts[0])\n",
        "            all_class_ids.append(class_id)\n",
        "            if class_id == cell_class_id:\n",
        "                has_cell_tower = True\n",
        "            if class_id in non_cell_class_ids:\n",
        "                has_non_cell = True\n",
        "\n",
        "    # Priority: cell tower > non-cell > other\n",
        "    if has_cell_tower:\n",
        "        return 'cell'\n",
        "    elif has_non_cell:\n",
        "        return 'non_cell'\n",
        "    else:\n",
        "        return 'unlabeled'\n",
        "\n",
        "def inspect_labels(data_dir, num_samples=10):\n",
        "    \"\"\"Inspect a few label files to see what's inside\"\"\"\n",
        "    train_labels_dir = os.path.join(data_dir, 'train', 'labels')\n",
        "\n",
        "    if not os.path.exists(train_labels_dir):\n",
        "        print(f\"Labels directory not found: {train_labels_dir}\")\n",
        "        return\n",
        "\n",
        "    label_files = [f for f in os.listdir(train_labels_dir) if f.endswith('.txt')][:num_samples]\n",
        "\n",
        "    print(f\"\\n=== INSPECTING {num_samples} LABEL FILES ===\")\n",
        "    for label_file in label_files:\n",
        "        label_path = os.path.join(train_labels_dir, label_file)\n",
        "        print(f\"\\nFile: {label_file}\")\n",
        "        with open(label_path, 'r') as f:\n",
        "            content = f.read()\n",
        "            if content.strip():\n",
        "                print(f\"Content: {content.strip()}\")\n",
        "            else:\n",
        "                print(\"Content: EMPTY\")\n",
        "\n",
        "def filter_dataset(data_dir, class_names):\n",
        "    \"\"\"\n",
        "    Filter dataset:\n",
        "    - TRAIN: Only use labeled images (cell=1, non/pow=0) for training\n",
        "    - VALID/TEST: Keep ALL images for prediction (will be labeled by model)\n",
        "\n",
        "    Args:\n",
        "        data_dir: Root directory of the dataset (should contain train/valid/test)\n",
        "        class_names: Dictionary mapping class IDs to names\n",
        "\n",
        "    Returns:\n",
        "        Training data and unlabeled validation/test data\n",
        "    \"\"\"\n",
        "    cell_class_id = None\n",
        "    non_cell_class_ids = []\n",
        "\n",
        "    # Find class IDs\n",
        "    for class_id, name in class_names.items():\n",
        "        if name.lower() == 'cell':\n",
        "            cell_class_id = class_id\n",
        "        elif name.lower() in ['non', 'pow']:\n",
        "            non_cell_class_ids.append(class_id)\n",
        "\n",
        "    print(f\"Cell tower class ID: {cell_class_id}\")\n",
        "    print(f\"Non-cell tower class IDs: {non_cell_class_ids}\")\n",
        "\n",
        "    # Training data (labeled only)\n",
        "    train_data = {'images': [], 'labels': []}\n",
        "\n",
        "    # Unlabeled data from train (other classes)\n",
        "    train_unlabeled = []\n",
        "\n",
        "    # All validation and test images (to be predicted)\n",
        "    valid_images = []\n",
        "    test_images = []\n",
        "\n",
        "    # Debugging counters\n",
        "    debug_stats = {\n",
        "        'train': {'total': 0, 'cell': 0, 'non_cell': 0, 'unlabeled': 0},\n",
        "        'valid': {'total': 0},\n",
        "        'test': {'total': 0}\n",
        "    }\n",
        "\n",
        "    # Process TRAIN split - filter for labeled data only\n",
        "    images_dir = os.path.join(data_dir, 'train', 'images')\n",
        "    labels_dir = os.path.join(data_dir, 'train', 'labels')\n",
        "\n",
        "    if os.path.exists(images_dir):\n",
        "        for img_file in os.listdir(images_dir):\n",
        "            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                continue\n",
        "\n",
        "            debug_stats['train']['total'] += 1\n",
        "\n",
        "            img_path = os.path.join(images_dir, img_file)\n",
        "            label_file = os.path.splitext(img_file)[0] + '.txt'\n",
        "            label_path = os.path.join(labels_dir, label_file)\n",
        "\n",
        "            label_type = parse_yolo_labels(label_path, cell_class_id, non_cell_class_ids)\n",
        "\n",
        "            if label_type == 'cell':\n",
        "                debug_stats['train']['cell'] += 1\n",
        "                train_data['images'].append(img_path)\n",
        "                train_data['labels'].append(1)  # Has cell tower\n",
        "            elif label_type == 'non_cell':\n",
        "                debug_stats['train']['non_cell'] += 1\n",
        "                train_data['images'].append(img_path)\n",
        "                train_data['labels'].append(0)  # No cell tower\n",
        "            else:\n",
        "                debug_stats['train']['unlabeled'] += 1\n",
        "                train_unlabeled.append(img_path)\n",
        "\n",
        "    # Process VALID split - keep ALL images for prediction\n",
        "    valid_images_dir = os.path.join(data_dir, 'valid', 'images')\n",
        "    if os.path.exists(valid_images_dir):\n",
        "        for img_file in os.listdir(valid_images_dir):\n",
        "            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                debug_stats['valid']['total'] += 1\n",
        "                valid_images.append(os.path.join(valid_images_dir, img_file))\n",
        "\n",
        "    # Process TEST split - keep ALL images for prediction\n",
        "    test_images_dir = os.path.join(data_dir, 'test', 'images')\n",
        "    if os.path.exists(test_images_dir):\n",
        "        for img_file in os.listdir(test_images_dir):\n",
        "            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                debug_stats['test']['total'] += 1\n",
        "                test_images.append(os.path.join(test_images_dir, img_file))\n",
        "\n",
        "    # Print detailed statistics\n",
        "    print(\"\\n=== DATASET STATISTICS ===\")\n",
        "    print(f\"\\nTRAIN (for training):\")\n",
        "    print(f\"  Total images: {debug_stats['train']['total']}\")\n",
        "    print(f\"  Cell tower images (label=1): {debug_stats['train']['cell']}\")\n",
        "    print(f\"  Non-cell tower images (label=0): {debug_stats['train']['non_cell']}\")\n",
        "    print(f\"  Unlabeled (other classes): {debug_stats['train']['unlabeled']}\")\n",
        "    print(f\"  → Training set size: {len(train_data['images'])}\")\n",
        "\n",
        "    print(f\"\\nVALID (for prediction):\")\n",
        "    print(f\"  Total images: {debug_stats['valid']['total']}\")\n",
        "    print(f\"  → All will be predicted by model\")\n",
        "\n",
        "    print(f\"\\nTEST (for prediction):\")\n",
        "    print(f\"  Total images: {debug_stats['test']['total']}\")\n",
        "    print(f\"  → All will be predicted by model\")\n",
        "\n",
        "    return train_data, train_unlabeled, valid_images, test_images\n",
        "\n",
        "def get_transforms(is_training=True):\n",
        "    \"\"\"Get image transformations for training and validation\"\"\"\n",
        "    if is_training:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(15),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "def create_model(num_classes=2):\n",
        "    \"\"\"Create a ResNet50 model for binary classification\"\"\"\n",
        "    model = models.resnet50(pretrained=True)\n",
        "\n",
        "    # Freeze early layers\n",
        "    for param in list(model.parameters())[:-15]:\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace final layer\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(num_features, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(256, num_classes)\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=20, device='cuda'):\n",
        "    \"\"\"Train the classification model\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                       factor=0.5, patience=3)\n",
        "\n",
        "    model = model.to(device)\n",
        "    best_val_acc = 0.0\n",
        "    has_validation = len(val_loader.dataset) > 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_preds = []\n",
        "        train_labels = []\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            train_preds.extend(preds.cpu().numpy())\n",
        "            train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc = accuracy_score(train_labels, train_preds)\n",
        "\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "\n",
        "        # Validation phase\n",
        "        if has_validation:\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_preds = []\n",
        "            val_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for images, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]'):\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    val_preds.extend(preds.cpu().numpy())\n",
        "                    val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_acc = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "            # Save best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(model.state_dict(), 'best_cell_tower_model.pth')\n",
        "                print(f'Best model saved with validation accuracy: {best_val_acc:.4f}')\n",
        "        else:\n",
        "            # Save model periodically when no validation\n",
        "            torch.save(model.state_dict(), 'best_cell_tower_model.pth')\n",
        "            print('Model saved (no validation set)')\n",
        "\n",
        "    return model\n",
        "\n",
        "def predict_unlabeled(model, image_paths, device='cuda', batch_size=32):\n",
        "    \"\"\"Predict labels for unlabeled images\"\"\"\n",
        "    transform = get_transforms(is_training=False)\n",
        "\n",
        "    # Create dataset without labels\n",
        "    class UnlabeledDataset(Dataset):\n",
        "        def __init__(self, image_paths, transform):\n",
        "            self.image_paths = image_paths\n",
        "            self.transform = transform\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.image_paths)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            img_path = self.image_paths[idx]\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, img_path\n",
        "\n",
        "    dataset = UnlabeledDataset(image_paths, transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    paths = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, img_paths in tqdm(loader, desc='Predicting images'):\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            paths.extend(img_paths)\n",
        "\n",
        "    return paths, predictions\n",
        "\n",
        "def remove_non_cell_tower_images(image_paths, predictions, split_name):\n",
        "    \"\"\"Remove images that don't contain cell towers\"\"\"\n",
        "    # Get the base directory\n",
        "    base_dir = os.path.dirname(os.path.dirname(image_paths[0]))\n",
        "    removed_dir = os.path.join(base_dir, f'removed_non_cell_towers_{split_name}')\n",
        "    os.makedirs(removed_dir, exist_ok=True)\n",
        "\n",
        "    removed_count = 0\n",
        "    kept_count = 0\n",
        "\n",
        "    for img_path, pred in zip(image_paths, predictions):\n",
        "        if pred == 0:  # No cell tower\n",
        "            # Move to removed directory\n",
        "            filename = os.path.basename(img_path)\n",
        "            dest_path = os.path.join(removed_dir, filename)\n",
        "            shutil.move(img_path, dest_path)\n",
        "\n",
        "            # Also remove corresponding label file if it exists\n",
        "            label_path = img_path.replace('/images/', '/labels/').replace('.jpg', '.txt').replace('.jpeg', '.txt').replace('.png', '.txt')\n",
        "            if os.path.exists(label_path):\n",
        "                label_dest = os.path.join(removed_dir, os.path.basename(label_path))\n",
        "                shutil.move(label_path, label_dest)\n",
        "\n",
        "            removed_count += 1\n",
        "        else:\n",
        "            kept_count += 1\n",
        "\n",
        "    print(f\"\\n{split_name.upper()} Cleanup:\")\n",
        "    print(f\"  Images kept (have cell towers): {kept_count}\")\n",
        "    print(f\"  Images removed (no cell towers): {removed_count}\")\n",
        "    print(f\"  Removed images saved to: {removed_dir}\")\n",
        "\n",
        "\n",
        "def clean_dataset_with_model(model_path, data_dir, device='cuda', batch_size=32):\n",
        "    \"\"\"\n",
        "    STANDALONE FUNCTION: Clean a dataset by removing non-cell tower images using a trained model.\n",
        "\n",
        "    This function can be used independently to clean any dataset with a pre-trained model.\n",
        "    It will:\n",
        "    1. Load the trained model from the given path\n",
        "    2. Scan through train/valid/test splits\n",
        "    3. Predict which images contain cell towers\n",
        "    4. Move non-cell tower images to separate folders\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the saved model weights (.pth file)\n",
        "        data_dir (str): Root directory of the dataset (should contain train/valid/test folders)\n",
        "        device (str): Device to run predictions on ('cuda' or 'cpu')\n",
        "        batch_size (int): Batch size for predictions\n",
        "\n",
        "    Returns:\n",
        "        dict: Statistics about the cleanup operation\n",
        "\n",
        "    Example usage:\n",
        "        >>> # Clean a new dataset using pre-trained weights\n",
        "        >>> stats = clean_dataset_with_model(\n",
        "        ...     model_path='best_cell_tower_model.pth',\n",
        "        ...     data_dir='/path/to/new/dataset',\n",
        "        ...     device='cuda',\n",
        "        ...     batch_size=32\n",
        "        ... )\n",
        "    \"\"\"\n",
        "    print('='*60)\n",
        "    print('DATASET CLEANUP WITH PRE-TRAINED MODEL')\n",
        "    print('='*60)\n",
        "    print(f'Model: {model_path}')\n",
        "    print(f'Dataset: {data_dir}')\n",
        "    print(f'Device: {device}')\n",
        "    print('='*60)\n",
        "\n",
        "    # Load the model\n",
        "    print('\\nLoading model...')\n",
        "    model = create_model(num_classes=2)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    print('Model loaded successfully!')\n",
        "\n",
        "    # Statistics dictionary\n",
        "    stats = {\n",
        "        'train': {'total': 0, 'kept': 0, 'removed': 0},\n",
        "        'valid': {'total': 0, 'kept': 0, 'removed': 0},\n",
        "        'test': {'total': 0, 'kept': 0, 'removed': 0}\n",
        "    }\n",
        "\n",
        "    # Process each split\n",
        "    splits = ['train', 'valid', 'test']\n",
        "\n",
        "    for split in splits:\n",
        "        print(f'\\n{\"=\"*60}')\n",
        "        print(f'Processing {split.upper()} split')\n",
        "        print(f'{\"=\"*60}')\n",
        "\n",
        "        images_dir = os.path.join(data_dir, split, 'images')\n",
        "\n",
        "        if not os.path.exists(images_dir):\n",
        "            print(f'  ⚠ Directory not found: {images_dir}')\n",
        "            continue\n",
        "\n",
        "        # Collect all images\n",
        "        image_paths = []\n",
        "        for img_file in os.listdir(images_dir):\n",
        "            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                image_paths.append(os.path.join(images_dir, img_file))\n",
        "\n",
        "        if len(image_paths) == 0:\n",
        "            print(f'  ⚠ No images found in {split} split')\n",
        "            continue\n",
        "\n",
        "        stats[split]['total'] = len(image_paths)\n",
        "        print(f'  Found {len(image_paths)} images')\n",
        "\n",
        "        # Predict\n",
        "        print(f'  Predicting...')\n",
        "        paths, predictions = predict_unlabeled(model, image_paths, device=device, batch_size=batch_size)\n",
        "\n",
        "        # Count predictions\n",
        "        cell_count = sum(1 for p in predictions if p == 1)\n",
        "        no_cell_count = sum(1 for p in predictions if p == 0)\n",
        "        print(f'  - With cell towers: {cell_count}')\n",
        "        print(f'  - Without cell towers: {no_cell_count}')\n",
        "\n",
        "        # Remove non-cell tower images\n",
        "        print(f'  Cleaning up...')\n",
        "        remove_non_cell_tower_images(paths, predictions, split)\n",
        "\n",
        "        stats[split]['kept'] = cell_count\n",
        "        stats[split]['removed'] = no_cell_count\n",
        "\n",
        "    # Print summary\n",
        "    print('\\n' + '='*60)\n",
        "    print('CLEANUP SUMMARY')\n",
        "    print('='*60)\n",
        "    total_kept = sum(stats[split]['kept'] for split in splits)\n",
        "    total_removed = sum(stats[split]['removed'] for split in splits)\n",
        "    total_processed = sum(stats[split]['total'] for split in splits)\n",
        "\n",
        "    for split in splits:\n",
        "        if stats[split]['total'] > 0:\n",
        "            print(f\"\\n{split.upper()}:\")\n",
        "            print(f\"  Total: {stats[split]['total']}\")\n",
        "            print(f\"  Kept: {stats[split]['kept']}\")\n",
        "            print(f\"  Removed: {stats[split]['removed']}\")\n",
        "\n",
        "    print(f\"\\nOVERALL:\")\n",
        "    print(f\"  Total processed: {total_processed}\")\n",
        "    print(f\"  Total kept: {total_kept}\")\n",
        "    print(f\"  Total removed: {total_removed}\")\n",
        "    print(f\"  Retention rate: {100*total_kept/total_processed:.1f}%\" if total_processed > 0 else \"  N/A\")\n",
        "\n",
        "    print('\\n' + '='*60)\n",
        "    print('✓ CLEANUP COMPLETE!')\n",
        "    print('='*60)\n",
        "    print('\\nNon-cell tower images have been moved to separate folders.')\n",
        "    print('Your train/valid/test folders now contain only images with cell towers.')\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    DATA_DIR = '/content/Bird-nest-dataset-1'\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_EPOCHS = 20\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    print(f'Using device: {DEVICE}')\n",
        "\n",
        "    # Load dataset configuration\n",
        "    yaml_path = os.path.join(DATA_DIR, 'data.yaml')\n",
        "    with open(yaml_path, 'r') as f:\n",
        "        data_config = yaml.safe_load(f)\n",
        "\n",
        "    # Get class names\n",
        "    class_names = {i: name for i, name in enumerate(data_config['names'])}\n",
        "    print(f'Classes: {class_names}')\n",
        "\n",
        "    # Inspect labels\n",
        "    inspect_labels(DATA_DIR, num_samples=10)\n",
        "\n",
        "    # Filter dataset\n",
        "    print('\\nFiltering dataset...')\n",
        "    train_data, train_unlabeled, valid_images, test_images = filter_dataset(DATA_DIR, class_names)\n",
        "\n",
        "    # Create validation set from training data\n",
        "    print('\\nCreating train/validation split from labeled training data...')\n",
        "\n",
        "    all_train_images = train_data['images']\n",
        "    all_train_labels = train_data['labels']\n",
        "\n",
        "    total_size = len(all_train_images)\n",
        "    val_size = int(0.2 * total_size)\n",
        "    train_size = total_size - val_size\n",
        "\n",
        "    indices = np.random.permutation(total_size)\n",
        "    train_indices = indices[:train_size]\n",
        "    val_indices = indices[train_size:]\n",
        "\n",
        "    train_images = [all_train_images[i] for i in train_indices]\n",
        "    train_labels = [all_train_labels[i] for i in train_indices]\n",
        "    val_images_for_training = [all_train_images[i] for i in val_indices]\n",
        "    val_labels_for_training = [all_train_labels[i] for i in val_indices]\n",
        "\n",
        "    print(f'Split - train: {len(train_images)}, validation: {len(val_images_for_training)}')\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = CellTowerDataset(\n",
        "        train_images,\n",
        "        train_labels,\n",
        "        transform=get_transforms(is_training=True)\n",
        "    )\n",
        "\n",
        "    val_dataset = CellTowerDataset(\n",
        "        val_images_for_training,\n",
        "        val_labels_for_training,\n",
        "        transform=get_transforms(is_training=False)\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                            shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                          shuffle=False, num_workers=2)\n",
        "\n",
        "    # Create and train model\n",
        "    print('\\nCreating model...')\n",
        "    model = create_model(num_classes=2)\n",
        "\n",
        "    print('\\nTraining model...')\n",
        "    model = train_model(model, train_loader, val_loader,\n",
        "                       num_epochs=NUM_EPOCHS, device=DEVICE)\n",
        "\n",
        "    # Clean the dataset using the trained model\n",
        "    print('\\n\\nNow cleaning the dataset with the trained model...')\n",
        "    stats = clean_dataset_with_model(\n",
        "        model_path='best_cell_tower_model.pth',\n",
        "        data_dir=DATA_DIR,\n",
        "        device=DEVICE,\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "NVqqXRIw8rAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6b231e4-97c2-4ab8-b852-fb9f28be91ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Classes: {0: '0', 1: '1', 2: '10', 3: '11', 4: '12', 5: '13', 6: '14', 7: '2', 8: '4', 9: '5', 10: '6', 11: '7', 12: '9', 13: 'cell', 14: 'non', 15: 'pow'}\n",
            "\n",
            "=== INSPECTING 10 LABEL FILES ===\n",
            "\n",
            "File: 1120_jpg.rf.079335c0f3740cde1de7091a60894a0a.txt\n",
            "Content: 14 0.5006521739130435 0.5 0.9987137681159419 1\n",
            "0 0.5244565217391305 0.26150121065375304 0.06702898550724638 0.12106537530266344\n",
            "\n",
            "File: i243_jpg.rf.d5bdb91a05a51dea0e8d4f0f15ae614a.txt\n",
            "Content: 0 0.5556640625 0.4652777777777778 0.078125 0.09375\n",
            "13 0.3908203125 0.6590277777777778 0.683984375 0.6819444444444445\n",
            "\n",
            "File: 419aa31cf7fc9e6d_jpg.rf.03441b9fcfe71db41901fea09bf86550.txt\n",
            "Content: 0 0.48031496062992124 0.5078947368421053 0.6929133858267716 0.8157894736842105\n",
            "14 0.4686220472440945 0.5247894736842105 0.937244094488189 0.950421052631579\n",
            "\n",
            "File: 16468-1_jpg.rf.b5fe2f3d73a9510033bc402e0cf2f22e.txt\n",
            "Content: 7 0.215625 0.1421875 0.0828125 0.0671875\n",
            "7 0.21171875 0.31484375 0.0828125 0.0953125\n",
            "7 0.62109375 0.58125 0.05625 0.0703125\n",
            "7 0.70703125 0.48671875 0.0625 0.075\n",
            "14 0.5451406249999999 0.501046875 0.90971875 0.997921875\n",
            "\n",
            "File: 1003_jpg.rf.1b7689de8ae9371513c57a3a596bf107.txt\n",
            "Content: 13 0.5381 0.6416725978647687 0.9238 0.7166548042704626\n",
            "0 0.433 0.45373665480427045 0.082 0.060498220640569395\n",
            "\n",
            "File: balloon_90_jpg.rf.2c35dd2b8e905371dd0f33ca8fae6193.txt\n",
            "Content: 7 0.4808259587020649 0.125 0.31268436578171094 0.234375\n",
            "14 0.5 0.5 1 1\n",
            "\n",
            "File: 16682-1_jpg.rf.73c7af97efe20afa883bac2992d24dfe.txt\n",
            "Content: 14 0.5 0.5 1 1\n",
            "1 0.6421875 0.40390625 0.025 0.05625\n",
            "1 0.7046875 0.453125 0.028125 0.065625\n",
            "\n",
            "File: 1190_jpg.rf.271b3a232477c541bdc77d734b79580f.txt\n",
            "Content: 1 0.5921875 0.5199063231850117 0.678125 0.5386416861826698\n",
            "14 0.5 0.5 1 1\n",
            "\n",
            "File: 1002_jpg.rf.5cbe09ffbf7d9a0a12b52cd336bb985f.txt\n",
            "Content: 13 0.5158151609553479 0.558457249070632 0.8994600207684319 0.8830669144981412\n",
            "1 0.2757009345794392 0.19609665427509293 0.1111111111111111 0.22490706319702602\n",
            "0 0.3125649013499481 0.38104089219330856 0.2367601246105919 0.1895910780669145\n",
            "0 0.3826583592938733 0.8094795539033457 0.14434060228452752 0.12825278810408922\n",
            "\n",
            "File: 28368347_105120093085_4_jpg.rf.0b08f1869274144c085edc27c6f80329.txt\n",
            "Content: 14 0.5 0.5 1 1\n",
            "0 0.6032608695652174 0.5081521739130435 0.6195652173913043 0.7554347826086957\n",
            "\n",
            "Filtering dataset...\n",
            "Cell tower class ID: 13\n",
            "Non-cell tower class IDs: [14, 15]\n",
            "\n",
            "=== DATASET STATISTICS ===\n",
            "\n",
            "TRAIN (for training):\n",
            "  Total images: 499\n",
            "  Cell tower images (label=1): 137\n",
            "  Non-cell tower images (label=0): 362\n",
            "  Unlabeled (other classes): 0\n",
            "  → Training set size: 499\n",
            "\n",
            "VALID (for prediction):\n",
            "  Total images: 30\n",
            "  → All will be predicted by model\n",
            "\n",
            "TEST (for prediction):\n",
            "  Total images: 32\n",
            "  → All will be predicted by model\n",
            "\n",
            "Creating train/validation split from labeled training data...\n",
            "Split - train: 400, validation: 99\n",
            "\n",
            "Creating model...\n",
            "\n",
            "Training model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 [Train]: 100%|██████████| 13/13 [00:17<00:00,  1.36s/it, loss=0.189]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n",
            "Train Loss: 0.4119, Train Acc: 0.8350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 [Val]: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1051, Val Acc: 0.9596\n",
            "Best model saved with validation accuracy: 0.9596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 [Train]: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s, loss=0.013]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/20\n",
            "Train Loss: 0.1070, Train Acc: 0.9625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2195, Val Acc: 0.9495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20 [Train]: 100%|██████████| 13/13 [00:09<00:00,  1.41it/s, loss=0.00251]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/20\n",
            "Train Loss: 0.0489, Train Acc: 0.9850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.3104, Val Acc: 0.9394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20 [Train]: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s, loss=0.000739]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/20\n",
            "Train Loss: 0.0197, Train Acc: 0.9900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1731, Val Acc: 0.9596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20 [Train]: 100%|██████████| 13/13 [00:08<00:00,  1.47it/s, loss=0.0646]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/20\n",
            "Train Loss: 0.0183, Train Acc: 0.9925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1512, Val Acc: 0.9697\n",
            "Best model saved with validation accuracy: 0.9697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 [Train]: 100%|██████████| 13/13 [00:08<00:00,  1.57it/s, loss=0.00396]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/20\n",
            "Train Loss: 0.0155, Train Acc: 0.9975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2043, Val Acc: 0.9394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20 [Train]: 100%|██████████| 13/13 [00:08<00:00,  1.59it/s, loss=0.00247]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/20\n",
            "Train Loss: 0.0161, Train Acc: 0.9900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2568, Val Acc: 0.9293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20 [Train]: 100%|██████████| 13/13 [00:08<00:00,  1.56it/s, loss=0.000376]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/20\n",
            "Train Loss: 0.0102, Train Acc: 0.9950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1864, Val Acc: 0.9495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20 [Train]: 100%|██████████| 13/13 [00:09<00:00,  1.32it/s, loss=0.000212]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/20\n",
            "Train Loss: 0.0137, Train Acc: 0.9925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20 [Val]: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2349, Val Acc: 0.9495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20 [Train]: 100%|██████████| 13/13 [00:09<00:00,  1.39it/s, loss=0.0489]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/20\n",
            "Train Loss: 0.0047, Train Acc: 0.9975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2696, Val Acc: 0.9495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20 [Train]: 100%|██████████| 13/13 [00:09<00:00,  1.39it/s, loss=7.63e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/20\n",
            "Train Loss: 0.0078, Train Acc: 0.9975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2581, Val Acc: 0.9394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20 [Train]: 100%|██████████| 13/13 [00:09<00:00,  1.39it/s, loss=0.00042]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/20\n",
            "Train Loss: 0.0057, Train Acc: 0.9975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1765, Val Acc: 0.9394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20 [Train]: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s, loss=0.000813]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13/20\n",
            "Train Loss: 0.0120, Train Acc: 0.9925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1379, Val Acc: 0.9495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20 [Train]: 100%|██████████| 13/13 [00:09<00:00,  1.39it/s, loss=0.000838]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14/20\n",
            "Train Loss: 0.0190, Train Acc: 0.9900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1304, Val Acc: 0.9596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20 [Train]: 100%|██████████| 13/13 [00:09<00:00,  1.44it/s, loss=0.00202]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15/20\n",
            "Train Loss: 0.0019, Train Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1229, Val Acc: 0.9596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20 [Train]: 100%|██████████| 13/13 [00:08<00:00,  1.57it/s, loss=0.00136]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16/20\n",
            "Train Loss: 0.0092, Train Acc: 0.9975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20 [Val]: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1266, Val Acc: 0.9596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20 [Train]: 100%|██████████| 13/13 [00:08<00:00,  1.60it/s, loss=0.00221]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17/20\n",
            "Train Loss: 0.0031, Train Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20 [Val]: 100%|██████████| 4/4 [00:03<00:00,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1373, Val Acc: 0.9596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20 [Train]: 100%|██████████| 13/13 [00:07<00:00,  1.64it/s, loss=0.000609]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/20\n",
            "Train Loss: 0.0061, Train Acc: 0.9975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1387, Val Acc: 0.9495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20 [Train]: 100%|██████████| 13/13 [00:08<00:00,  1.45it/s, loss=0.00991]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/20\n",
            "Train Loss: 0.0026, Train Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1267, Val Acc: 0.9596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20 [Train]: 100%|██████████| 13/13 [00:09<00:00,  1.44it/s, loss=0.000407]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/20\n",
            "Train Loss: 0.0066, Train Acc: 0.9950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20 [Val]: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s]\n",
            "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1510, Val Acc: 0.9495\n",
            "\n",
            "\n",
            "Now cleaning the dataset with the trained model...\n",
            "============================================================\n",
            "DATASET CLEANUP WITH PRE-TRAINED MODEL\n",
            "============================================================\n",
            "Model: best_cell_tower_model.pth\n",
            "Dataset: /content/Bird-nest-dataset-1\n",
            "Device: cuda\n",
            "============================================================\n",
            "\n",
            "Loading model...\n",
            "Model loaded successfully!\n",
            "\n",
            "============================================================\n",
            "Processing TRAIN split\n",
            "============================================================\n",
            "  Found 499 images\n",
            "  Predicting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting images: 100%|██████████| 16/16 [00:10<00:00,  1.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - With cell towers: 139\n",
            "  - Without cell towers: 360\n",
            "  Cleaning up...\n",
            "\n",
            "TRAIN Cleanup:\n",
            "  Images kept (have cell towers): 139\n",
            "  Images removed (no cell towers): 360\n",
            "  Removed images saved to: /content/Bird-nest-dataset-1/train/removed_non_cell_towers_train\n",
            "\n",
            "============================================================\n",
            "Processing VALID split\n",
            "============================================================\n",
            "  Found 30 images\n",
            "  Predicting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting images: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - With cell towers: 30\n",
            "  - Without cell towers: 0\n",
            "  Cleaning up...\n",
            "\n",
            "VALID Cleanup:\n",
            "  Images kept (have cell towers): 30\n",
            "  Images removed (no cell towers): 0\n",
            "  Removed images saved to: /content/Bird-nest-dataset-1/valid/removed_non_cell_towers_valid\n",
            "\n",
            "============================================================\n",
            "Processing TEST split\n",
            "============================================================\n",
            "  Found 32 images\n",
            "  Predicting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting images: 100%|██████████| 1/1 [00:00<00:00,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - With cell towers: 30\n",
            "  - Without cell towers: 2\n",
            "  Cleaning up...\n",
            "\n",
            "TEST Cleanup:\n",
            "  Images kept (have cell towers): 30\n",
            "  Images removed (no cell towers): 2\n",
            "  Removed images saved to: /content/Bird-nest-dataset-1/test/removed_non_cell_towers_test\n",
            "\n",
            "============================================================\n",
            "CLEANUP SUMMARY\n",
            "============================================================\n",
            "\n",
            "TRAIN:\n",
            "  Total: 499\n",
            "  Kept: 139\n",
            "  Removed: 360\n",
            "\n",
            "VALID:\n",
            "  Total: 30\n",
            "  Kept: 30\n",
            "  Removed: 0\n",
            "\n",
            "TEST:\n",
            "  Total: 32\n",
            "  Kept: 30\n",
            "  Removed: 2\n",
            "\n",
            "OVERALL:\n",
            "  Total processed: 561\n",
            "  Total kept: 199\n",
            "  Total removed: 362\n",
            "  Retention rate: 35.5%\n",
            "\n",
            "============================================================\n",
            "✓ CLEANUP COMPLETE!\n",
            "============================================================\n",
            "\n",
            "Non-cell tower images have been moved to separate folders.\n",
            "Your train/valid/test folders now contain only images with cell towers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download the best model path\n",
        "\n",
        "from google.colab import files\n",
        "files.download('best_cell_tower_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "PNuJgp49vCcI",
        "outputId": "fee52ab3-85af-4f41-ef1c-9470b73e44ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b82704cd-5713-4cf2-ba01-825bcefe2db6\", \"best_cell_tower_model.pth\", 96455723)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}